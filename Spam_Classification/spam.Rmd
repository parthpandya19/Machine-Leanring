---
title: "R Notebook"
output: html_notebook
---

##PROBLEM 1

```{r}
sms_raw<- read.csv("C://Users//pandy//OneDrive//Documents//data_mining//spam.csv",stringsAsFactors = FALSE)
str(sms_raw) 
head(sms_raw)
sms_raw$type <-factor(sms_raw$type) 
```
Since the type feature is categorical varibale we convert it to factors
```{r}
str(sms_raw$type)
table(sms_raw$type)

#install.packages("tm")
library(NLP)
library(tm) #this package is useful to remove numbers and punctuation from strings amd handle uninteresting words and breaking sentences into meaningful individual words.
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
inspect(sms_corpus[1:2]) #gives summary of first two text messages
as.character(sms_corpus[[1]]) # to view the actual text message


```

```{r}
lapply(sms_corpus[1:2], as.character) #lapply() applies as.caracter function to required elements of the corpus. In this way we can select multiple text messages for viewing.

sms_corpus_clean <- tm_map(sms_corpus,content_transformer(tolower))

```
tm_map() is used for transforming the data with clean values i.e. getting rid of puntctuations
tolower() function returns a lowercase version of text strings. In order to apply this function to the corpus, we need to use the tm wrapper function content_transformer() to treat tolower() as a transformation function that can be used to access the corpus.

```{r}
as.character(sms_corpus_clean[[1]])
as.character(sms_corpus[[1]]) #confirming that the data is cleaned and changed to lower case text 
```

```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers) #removing numbers from the text
sms_corpus_clean <- tm_map(sms_corpus_clean,removeWords, stopwords()) #removing stopwords like to,and,for. stopwords() contain all the stopwords used in different languages. Default language is English. Removewords() will remove any stopwords which appear in the text messages
```

```{r}
sms_corpus_clean<-tm_map(sms_corpus_clean,removePunctuation)#remove punctuation
```

```{r}
#install.packages("SnowballC")
library(SnowballC)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
```
Stemming the document will reduce the understanding of words to a narrow list. For example, faster,fastest,fast will fall under one root word 'fast'. This is useful for machine learning algorithm to treat related words as a single concept.
```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
lapply(sms_corpus[1:2], as.character)
lapply(sms_corpus_clean[1:2], as.character) #testing the cleaning of data

```
Creating a Document Term Matrix where each row is a messgae and the columns are the elements (words)of the message.
```{r}
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
#sms_dtm<-DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE,
#                                                 removeNumbers = TRUE,
#                                                stopwords = TRUE,
#                                       removePunctuation = TRUE,
#                                         stemming = TRUE))
#we can use this function as well to the raw uncleaned data, which will perform all the tasks of cleaning.

```
Splitting the dataset 
```{r}
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]

sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
```

```{r}
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
```

```{r}
#install.packages("wordcloud")
library(RColorBrewer)
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
#wordcloud provides a cloud of words which appear most frequently in the corpus.
#random.order=FALSE specifies to display the cloud with highest frequency words in the center. min.freq=50 specifies the number of times the word appears to be displayed in the cloud i.e. a word should appear atleast 50 times to be displayed in the wordcloud

```

```{r}
spam <- subset(sms_raw, type == "spam")
ham<-subset(sms_raw,type=="ham")
```

```{r}
wordcloud(spam$text, max.words = 40, scale = c(4, 0.75),random.order = FALSE)
wordcloud(ham$text, max.words = 40, scale = c(4, 0.75),random.order=FALSE)
#created wordcloud for spam and ham, scaling as per visual comfort
```

```{r}
findFreqTerms(sms_dtm_train, 5) #finds words which apper in 5 messages
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
```
```{r}
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
```

```{r}
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
} #naive bayes classifies categorical features hence we need to convert the numeric features. This function converts the count of variable x to Yes/No strings. 
```

```{r}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts) #margin=2 specifies columns, apply() applies the changes to be made in rows or columns
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
```

```{r}
install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
classifier<-naiveBayes(seen,se)
preddd<-predict(classifier,dfs)
preddd

```

```{r}
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,prop.chisq = FALSE, prop.t = FALSE,dnn = c('predicted', 'actual'))
```
```{r}
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels,
laplace = 1)

sms_test_pred2<- predict(sms_classifier2,sms_test)

CrossTable(sms_test_pred2, sms_test_labels,prop.chisq = FALSE, prop.t = FALSE,dnn = c('predicted', 'actual'))
```
##Problem 2

```{r}
install.packages("klaR")
library(MASS)
library(klaR)
data(iris)


head(iris)

# identify indexes to be in testing dataset
# every index of 5th, 10th, 15th .. will be the testing dataset
# the rest are training dataset
testidx <- which(1:length(iris[, 1]) %% 5 == 0) 


# separate into training and testing datasets
iristrain <- iris[-testidx,] 
iristest <- iris[testidx,]

# applying Naive Bayes
nbmodel <- NaiveBayes(Species~., data=iristrain) #computes a-posterior probability of categorical class variable given  independent predictor variables

# check the accuracy
prediction <- predict(nbmodel, iristest[,-5]) 
table(prediction$class, iristest[,5]) 

```
How would you make a prediction for a new case with the above package?

We can create a vector for the new data values for example x. In the predict function 
we can write predict("trained model",x)

How does this package deal with numeric features? 
For each numeric variable, we get a table, for each target class, mean and
standard deviation of the (sub-)variable or a object of class density.


How does it specify a Laplace estimator?

The default value for the estimator is set to zero. We can change the value for the estimator  by nbmodel <- NaiveBayes(Species~., data=iristrain,laplace="desired value")



##Problem 3

We use laplace estimators in Naive Bayes classification because we want to retain all the information available for classification. Some features may have unknown values or which do not appear in search results cannot be discarded directly as it can cause loss of information while classifying data.
**Example**
Suppose there are two classes X,Y which have three features A,B and C
x: A=1,B=0,C=3
Y: A=0,B=2,C=1
so in this case B and A do not appear hence they are discarded from classification 
If we get a new case where
B=1,c=2 
We will not be able to classify it as X or Y as B has been discarded.
Hence if we use laplace estimator it assigns each feature an indisputable value which will include the features for classification.
In this example if we assign laplace estimator as 1 then it will assign 1 to unknown features
x: A=1 B=1 c=3
